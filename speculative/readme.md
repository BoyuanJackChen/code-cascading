# Batched Speculative Inference of LLM

Don't expect accuracy with fp16.

## TODO: 
Create the full prompt providing pipeline - alpaca + starter_ids + prompt
Edit generator.py to make it support multi-gpu
Implement early stopping